[
    {
      "title": "Holiday Release 12\/27\/2019",
      "content": "<p>Here&#39;s the list of bug fixes and improvements for the release<\/p>\n\n<p>1. users page show the correct number of challenges in progress<\/p>\n\n<p>2. fixed bug when&nbsp;empty task caused save error<\/p>\n\n<p>3. fixed bug task manager shows scrolling when there are a lot of links to a task<\/p>\n",
      "author": "peter.lin",
      "createDate": "1577109789000",
      "lastUpdate": "1577573025000"
    },
    {
      "title": "Cleaned + stop words == joy",
      "content": "<p>After a couple of hours hacking, I finally got the journal entry JSON data converted to tokens. The hard part was figuring out why my StopWord function was doing bad things. At first I was puzzled why there was a bunch of null objects in the result and saving the tokens was failing. Two things helped me diagnose the problem.<\/p>\n\n<p>1. set the logging level to debug by calling sc.setLogLevel(&quot;DEBUG&quot;)<\/p>\n\n<p>2. adding logs to my function<\/p>\n\n<p>When I copied the code from my utility, I forgot to change how it loads the english stop words. Originally I had it load from the jar file using the classloader. For Spark shell that doesn&#39;t make much sense and it makes more sense to pass in the location of the file. Once I fixed that bug, I was able to test the function. I did in shell with<\/p>\n\n<p>:require giildutils.jar<\/p>\n\n<p>import net.giild.text.{StripHtml,StopWord}<\/p>\n\n<p>val func = new StopWord()<\/p>\n\n<p>With debugging level, I could see it loaded the stop words. How do you use the StopWord function? The way it works, you want to use it after splitting the text into individual words. I will include a sample JSON in the github repo along with a working example.<\/p>\n\n<p><img src=\"https:\/\/s3.amazonaws.com\/media.giild.net\/user\/88533e92-4806-4b8a-b7ff-254768be02c7\/images\/spark-shell-stop.png\" \/><\/p>\n\n<p>Notice there are rows that are empty. This is because StopWord replaced it with empty string<\/p>\n",
      "author": "peter.lin",
      "createDate": "1577552177000",
      "lastUpdate": "1577552177000"
    },
    {
      "title": "Concatenating columns",
      "content": "<p>I need to concatenate the title, content and author for each entry before doing map\/reduce. Sadly, Apache Spark docs don&#39;t provide a simple example of how to do it. Here is a simple example.<\/p>\n\n<p>import org.apache.spark.sql.functions.{concat,lit}<\/p>\n\n<p>val rawdata = dataset.select( concat(col(&quot;title&quot;), lit(&quot; &quot;), col(&quot;content&quot;)))<\/p>\n\n<p>Concat is the name of the function<\/p>\n\n<p>Lit is for string literal<\/p>\n\n<p>Col tells Spark you want a column<\/p>\n\n<p>The command tells Spark to select title and content columns and concatenate with a space.<\/p>\n\n<p><img src=\"https:\/\/s3.amazonaws.com\/media.giild.net\/user\/88533e92-4806-4b8a-b7ff-254768be02c7\/images\/spark-shell-concat.png\" \/><\/p>\n",
      "author": "peter.lin",
      "createDate": "1577542546000",
      "lastUpdate": "1577549719000"
    },
    {
      "title": "I love open source and I'm lazy",
      "content": "<p>Today I spent several hours remembering how to use Spark shell. The thing is, if I don&#39;t use something for more than a year, it takes a while to remember. Something as simple as writing a function for Spark took a bit of digging to find the javadocs. Writing the function was pretty quick, but remembering how to use it in Spark shell took 3x longer. The good news is my function for stripping HTML tags works and I can use it from the shell.<\/p>\n\n<p>Rather than forget this 2 months from now, I decided it would better to open source it up on github. Here is the URL for it. Right now there&#39;s only 1 function, but I will add more later on.<\/p>\n\n<p><a href=\"https:\/\/github.com\/giild\/sparklyGiiLD\">sparklyGiiLD<\/a><\/p>\n\n<p>To use it, I started up Spark shell and ran some commands<\/p>\n\n<p>val path = &quot;\/home\/peter\/Documents\/entry_rows.json&quot;<\/p>\n\n<p>val dataset = spark.read.json(spark.sparkContext.wholeTextFiles(path).values);<\/p>\n\n<p>val rawtext = dataset.select(&quot;content&quot;);<\/p>\n\n<p>:require giildutils.jar<\/p>\n\n<p>import net.giild.text.StripHtml<\/p>\n\n<p>val sanitized = rawtext.map(x =&gt; new StripHtml().call(x.getString(0));<\/p>\n\n<p>santized.show();<\/p>\n\n<p>val words = sanitized.flatMap(x =&gt; x.split(&quot; &quot;));<\/p>\n\n<p>words.show();<\/p>\n\n<p><img src=\"https:\/\/s3.amazonaws.com\/media.giild.net\/user\/88533e92-4806-4b8a-b7ff-254768be02c7\/images\/spark-sanitized-words.png\" \/><\/p>\n\n<p>&nbsp;<\/p>\n",
      "author": "peter.lin",
      "createDate": "1577497768000",
      "lastUpdate": "1577497768000"
    },
    {
      "title": "cleaning up HTML tags",
      "content": "<p>In the previous entry, I included a picture of the output. The thing about generating autocomplete tokens from journal entries is punctuation and HTML tags.<\/p>\n\n<p><img src=\"https:\/\/s3.amazonaws.com\/media.giild.net\/user\/88533e92-4806-4b8a-b7ff-254768be02c7\/images\/spark-unclean-words.png\" \/><\/p>\n\n<p>To get an accurate count of the words, the program needs to strip out all of the punctuation marks and html tags. Rather than write one from scratch, I thought it would be good to search for an existing tool.<\/p>\n\n<p>I googled with the following keywords\/phrases with useless results:<\/p>\n\n<p>html scraper<\/p>\n\n<p>java html scraper<\/p>\n\n<p>java strip html tags from text<\/p>\n\n<p>html cleaner<\/p>\n\n<p>After an hour of googling, I decided it would be a waste of time to google any further. Google did return&nbsp;results that could strip out the HTML tags, but they weren&#39;t java libraries I could easily use in Spark.<\/p>\n\n<p>Over the last 15 years, I&#39;ve used a variety of HTML parsers like JSoup, HtmlParser, Tidy and a few others. Most of them parse HTML to a document model. Once you have the document,&nbsp;you have to iterate over the elements to extract the raw text. It&#39;s kind of tedious and memory intensive. I was hoping for a simple parser that would take HTML input and output raw text. Google did return a bunch of results for stackoverflow where people asked similar questions.<\/p>\n\n<p>Earlier in the summer, I wrote&nbsp;a simple class to strip out HTML with regular expressions. It was meant to be a quick and dirty utility. Since there aren&#39;t any existing libraries, I&#39;m going to use my little utility in spark.<\/p>\n",
      "author": "peter.lin",
      "createDate": "1577469090000",
      "lastUpdate": "1577469090000"
    },
    {
      "title": "Reading journal entry data: easy and hard",
      "content": "<p>1. start spark with &quot;spark-shell&quot;<\/p>\n\n<p>2. val path = &quot;\/home\/peter\/Documents\/entry_data.json&quot;;<\/p>\n\n<p>3. val dataset = spark.read.json(spark.sparkContext.wholeTextFiles(path).values);<\/p>\n\n<p>4. dataset.printSchema();<\/p>\n\n<p><img src=\"https:\/\/s3.amazonaws.com\/media.giild.net\/user\/88533e92-4806-4b8a-b7ff-254768be02c7\/images\/spark-nested-array.png\" \/><\/p>\n\n<p>The sample data I&#39;m using is from our REST service, which has array of entry objects associated with entries field. If you use nested array, it makes it harder to iterate over the array. It&#39;s easier if the JSON just has array of records like this:<\/p>\n\n<p><img src=\"https:\/\/s3.amazonaws.com\/media.giild.net\/user\/88533e92-4806-4b8a-b7ff-254768be02c7\/images\/entry_rows_json.png\" \/><\/p>\n\n<p>Ideally, after you&#39;ve ETL the data, it should be saved as normal format with 1 line per record and no comma at the end. That makes it easy for Spark and Hadoop to split the file.<\/p>\n\n<p>Using the second format for the journal entries, you can split the words like this.<\/p>\n\n<p>val datarows = dataset.select(&quot;content&#39;);<\/p>\n\n<p>val words = datarows.flatMap(x =&gt; x.getString(0).split(&quot; &quot;));<\/p>\n\n<p>To save the words to a file:<\/p>\n\n<p>words.write.format(&quot;csv&quot;).save(&quot;words&quot;);<\/p>\n\n<p>Once it is done saving the words, there should be a folder named &quot;words&quot;. In the folder will be one or more files.<\/p>\n\n<p><img src=\"https:\/\/s3.amazonaws.com\/media.giild.net\/user\/88533e92-4806-4b8a-b7ff-254768be02c7\/images\/spark-shell-words-output.png\" \/><\/p>\n",
      "author": "peter.lin",
      "createDate": "1577333382000",
      "lastUpdate": "1577458513000"
    },
    {
      "title": "Spark cube isn't a MOLAP",
      "content": "<p>This morning I was playing around with Spark SQL and reading the javadocs.&nbsp;<\/p>\n\n<p><a href=\"https:\/\/spark.apache.org\/docs\/1.6.3\/api\/java\/org\/apache\/spark\/sql\/DataFrame.html#select(org.apache.spark.sql.Column...)\" target=\"_blank\">spark sql javadocs<\/a><\/p>\n\n<p>I thought the javadoc explanation of cube was interesting:<\/p>\n\n<p><em>Create a multi-dimensional cube for the current&nbsp;<a href=\"https:\/\/spark.apache.org\/docs\/1.6.3\/api\/java\/org\/apache\/spark\/sql\/DataFrame.html\" target=\"_blank\" title=\"class in org.apache.spark.sql\"><code>DataFrame<\/code><\/a>&nbsp;using the specified columns, so we can run aggregation on them. See&nbsp;<a href=\"https:\/\/spark.apache.org\/docs\/1.6.3\/api\/java\/org\/apache\/spark\/sql\/GroupedData.html\" target=\"_blank\" title=\"class in org.apache.spark.sql\"><code>GroupedData<\/code><\/a>&nbsp;for all the available aggregate functions.&nbsp;This is a variant of cube that can only group by existing columns using column names (i.e. cannot construct expressions).<\/em><\/p>\n\n<p>In theory, you could use the join() function to combine multiple DataFrames before grouping the data by specific columns. In database terms, this is a summary table that groups columns for easier calculations. A more advanced version of cube is used in OLAP products like analysis service. Generally speaking, OLAP cubes have dimensions and measures. A dimension maps to a column. A column can be a physical column or the result of a function. A measure is a calculation like sum, avg, mean, min or max.<\/p>\n\n<p>The reason cube feature is available in Spark, Hive and other query engines is simple calculations with grouping. The common example is:<\/p>\n\n<p>calculate the total sales by month for each state<\/p>\n\n<ul>\n\t<li>the dimensions are month and state<\/li>\n\t<li>the measure is sum<\/li>\n<\/ul>\n\n<p>What if we want to know more granular details like:<\/p>\n\n<p>calculate the average sale for each product category, sub category by county, state and month<\/p>\n\n<p>To produce the result for this kind of report, you&#39;d need to get data from all the product tables, sales tables and geography tables for counties. This is what&#39;s called a star and snowflake schema. Depending on how you model the cube, it can be a star or snowflake.<\/p>\n\n<p><a href=\"https:\/\/en.wikipedia.org\/wiki\/Snowflake_schema\" target=\"_blank\">wikipedia definition of snowflake schema<\/a><\/p>\n\n<p>On a practial level, OLAP products provide a visual editor to define the star\/snowflake schema. Usually, the data is extracted, transformed and loaded into the OLAP database. To do the same thing in Spark Sql would be kind of painful. To do cubes, it makes sense to use other tools. Apache kylin integrates with spark and&nbsp;has been around for a while.<\/p>\n\n<p><a href=\"http:\/\/kylin.apache.org\/docs\/tutorial\/cube_build_job.html\" target=\"_blank\">apache kylin cubes<\/a><\/p>\n\n<p>The other benefit of using OLAP cubes is performance. Most OLAP database use bitmap indexes to speed up queries. Some OLAP products pre-calculate the measures when it builds the cube, but that has the problem of space explosion. Space explosion just means it uses a ton of memory or disk space.<\/p>\n",
      "author": "peter.lin",
      "createDate": "1577369444000",
      "lastUpdate": "1577375922000"
    },
    {
      "title": "Spark can't read top level array",
      "content": "<p>The way spark works, it expects json file to have 1 entry per line without a comma at the end. This is similar to how Hadoop reads CSV files. It&#39;s necessary for a cluster to split a file and send chunks&nbsp;to multiple nodes. It&#39;s how work is divided across a cluster. Since I am using a sample from our REST service, it spans multiple lines. To get around that, you have to use spark.read.json(spark.sparkContext.wholeTextFile(path).values)<\/p>\n\n<p><img src=\"https:\/\/s3.amazonaws.com\/media.giild.net\/user\/88533e92-4806-4b8a-b7ff-254768be02c7\/images\/spark-shell-read-entries.png\" \/><\/p>\n\n<p>Once the file is loaded, you can iterate over the records. In my case, I can iterate over the entries.<\/p>\n",
      "author": "peter.lin",
      "createDate": "1577324533000",
      "lastUpdate": "1577371461000"
    },
    {
      "title": "Playing around with spark",
      "content": "<p>The last time I used spark was over 2 years back. For me, playing with technology is what I do during vacations and holidays. I don&#39;t consider it work and there&#39;s nothing I&#39;d rather do over winter break. If it was sunny, I&#39;d go out for a 60 mile ride, but it&#39;s freezing outside. Plus, I have Zwift, which makes the trainer fun.<\/p>\n\n<p>I started the autocomplete challenge to improve search.&nbsp; Before I can get a real sense of the improvement to search, I need real tokens. That means I need to process our data to extract the subjects and objects. Basically, it means doing natural language processing.<\/p>\n\n<p>An easier first step is to parse the text to extract the nouns. The easiest way to do this is with word count and filter it with skip words. For those new to search technology, skip words are used to skip common words.&nbsp;<\/p>\n\n<p><a href=\"https:\/\/github.com\/apache\/lucene-solr\/blob\/master\/solr\/core\/src\/test-files\/solr\/collection1\/conf\/stopwords.txt\">apache lucene solr stopword<\/a><\/p>\n\n<p>The reason search engines use skip words, is that pronouns and other common words aren&#39;t useful for search. Strictly speaking, there are cases where skip words are important, but for 95% of the use cases they aren&#39;t useful.<\/p>\n\n<p>The benefit of using Spark is it comes with a shell. You can easily do a word count test with spark without opening Eclipse or VisualStudios. I&#39;m a big fan of shells. From Unix to Spark to Inference rule engines, shells are super useful.<\/p>\n\n<p>Once a get a basic set of tokens for journal entries, bookmarks and courses, I can test out autocomplete.<\/p>\n\n<p><a href=\"https:\/\/databricks.com\/blog\/2015\/02\/02\/an-introduction-to-json-support-in-spark-sql.html\">JSON Spark Sql<\/a><\/p>\n",
      "author": "peter.lin",
      "createDate": "1577286241000",
      "lastUpdate": "1577371389000"
    },
    {
      "title": "Installing Spark locally in Linux",
      "content": "<p>Today I tried to install the latest 2.4.4 release on windows and found it had issues. Since I have a few linux servers, I decided to install it on my server instead. This page was a good reference.<\/p>\n\n<p><a href=\"https:\/\/computingforgeeks.com\/how-to-install-apache-spark-on-ubuntu-debian\/\">manually installing spark<\/a><\/p>\n\n<p><a href=\"https:\/\/archive.apache.org\/dist\/spark\/spark-2.4.4\/\">spark 2.4.4 index<\/a><\/p>\n\n<p>Here&#39;s the steps I used:<\/p>\n\n<p>1. use curl to download the release. in my case it was curl -O https:\/\/archive.apache.org\/dist\/spark\/spark-2.4.4\/spark-2.4.4-bin-hadoop2.7.tgz<\/p>\n\n<p>2. unpack the file with &quot;tar -xvf spark-2.4.4-bin-hadoop2.7.tgz&quot;<\/p>\n\n<p>3. move the folder with &quot;sudo mv spark-2.4.4-bin-hadoop2.7\/ \/opt\/spark&quot;<\/p>\n\n<p>4. update bash shell with &quot;vi ~\/.bashrc&quot;<\/p>\n\n<p>5. add these two lines to the shell at the end<\/p>\n\n<p>export SPARK_HOME=\/opt\/spark<br \/>\nexport PATH=$PATH:$SPARK_HOME\/bin:$SPARK_HOME\/sbin<\/p>\n\n<p>6. save the changes<\/p>\n\n<p>To test it installed correctly, I begin with starting the master server<\/p>\n\n<p>start-master.sh<\/p>\n\n<p>Then I start spark shell<\/p>\n\n<p>spark-shell<\/p>\n\n<p><img src=\"https:\/\/s3.amazonaws.com\/media.giild.net\/user\/88533e92-4806-4b8a-b7ff-254768be02c7\/images\/spark-shell-linux.png\" \/><\/p>\n\n<p>To make sure things work, I tried to load a sample json file<\/p>\n\n<p>val path=&quot;\/home\/peter\/Documents\/test_data.json&quot;<\/p>\n\n<p>val testdata = spark.read.json(path)<\/p>\n\n<p><img src=\"https:\/\/s3.amazonaws.com\/media.giild.net\/user\/88533e92-4806-4b8a-b7ff-254768be02c7\/images\/spark-shell-readata.png\" \/><\/p>\n",
      "author": "peter.lin",
      "createDate": "1577323423000",
      "lastUpdate": "1577371359000"
    },      {
      "title": "Pathway sorting tasks",
      "content": "<p>Now that we have a decent task manager, one side effect is &quot;how do you sort the order of the tasks?&quot;<\/p>\n\n<p>A side question is &quot;does the order of the task matter?&quot;<\/p>\n\n<p>Depending on the situation, the sort order might be important. Right now the pathway logic sorts based on the timestamp. But which timestamp should it use? Each task has a create, start and finish timestamp. Currently, it uses the create timestamp to sort the user tasks.<\/p>\n\n<p>Each task has a sort order that is left over from early in June 2019. The pathway layout logic doesn&#39;t use the sort order field. It&#39;s a little complicated. In the task manager, we have 3 columns: ideas, in progress, complete. Should we sort the tasks in this order: completed, in progress and ideas? Sometimes that the order you want, but other times it isn&#39;t.<\/p>\n\n<p>There is no perfect solution to this tricky problem. Ultimately, the visual editor will need to provide a way for users to order the tasks. This means we need a service that takes the full list of&nbsp;task ID and sort order. Update all of them to make sure it is correct. This way the graph logic in the service can sort the tasks by sortOrder first and then arbitrarily set the timestamp so the layout logic puts the in the correct order.<\/p>\n\n<p>The pathway never shows the timestamp, so it&#39;s shouldn&#39;t cause confusion. At least that&#39;s the theory I am leaning towards right now.<\/p>\n",
      "author": "peter.lin",
      "createDate": "1577213278000",
      "lastUpdate": "1577213278000"
    }
]
